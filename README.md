# Прототип системы анализа доходов фрилансеров с использованием LLM

Этот проект представляет собой прототип системы, которая анализирует статистические данные о доходах фрилансеров и предоставляет ответы на запросы, сформулированные на естественном языке. Система использует большую языковую модель (LLM) для интерпретации запросов пользователя и выполняет анализ данных с помощью Python и Pandas.

**Ссылка на датасет:** [Freelancer Earnings and Job Trends on Kaggle](https://www.kaggle.com/datasets/shohinurpervezshohan/freelancer-earnings-and-job-trends?select=freelancer_earnings_bd.csv) (использовался файл `freelancer_earnings_bd.csv`)

## Особенности

*   **Интерфейс командной строки (CLI):** Взаимодействие с системой происходит через текстовые команды.
*   **Обработка запросов на естественном языке:** Пользователь может задавать вопросы так, как если бы он общался с аналитиком.
*   **Интеграция с LLM (DeepSeek API):** LLM используется для преобразования естественного языка в структурированные JSON-команды.
*   **Анализ данных без прямой загрузки в LLM:** Сам датасет не передается в LLM, что обеспечивает конфиденциальность данных и точность вычислений.
*   **Модульная структура:** Логика взаимодействия с LLM и логика анализа данных разделены.

## Подход к решению задачи

Выбранный подход заключается в создании гибридной системы, где LLM выступает в роли интерпретатора запросов, а не прямого анализатора данных:

1.  **Пользовательский ввод:** Через CLI пользователь вводит вопрос.
2.  **Интерпретация LLM:** Вопрос вместе с детальным системным промптом (содержащим схему данных и описание возможных аналитических операций) отправляется в DeepSeek API. LLM генерирует JSON-команду.
3.  **Выполнение анализа:** Python-скрипт парсит JSON и вызывает соответствующую функцию анализа данных (написанную с использованием Pandas), которая работает с локально загруженным датасетом.
4.  **Вывод результата:** Результат анализа отображается пользователю.

Этот подход позволяет сочетать гибкость понимания естественного языка LLM с точностью и контролируемостью вычислений на Python.

### Альтернативный подход: Использование RAG (Retrieval Augmented Generation)

Хотя в данном прототипе LLM не работает напрямую с содержимым CSV, а только с его схемой, задачу можно было бы также решать с использованием **Retrieval Augmented Generation (RAG)**. Это было бы особенно полезно, если бы:

*   Структура датасета была бы менее формализована или содержала много текстовых описаний, которые нужно учитывать.
*   Требовалось бы отвечать на вопросы, требующие извлечения конкретных фактов или строк из данных, а не только агрегированной статистики.
*   Нужно было бы динамически адаптироваться к изменениям в датасете без переписывания промптов для каждой новой колонки.

**Как бы выглядела система с RAG (концептуально):**

1.  **Индексация данных:**
    *   Датасет (или его релевантные части, возможно, предварительно обработанные) разбивается на более мелкие фрагменты (чанки).
    *   Для каждого чанка создается векторное представление (embedding) с помощью какой-либо модели эмбеддингов (например, Sentence Transformers, OpenAI Embeddings, или эмбеддинги от DeepSeek, если доступны).
    *   Эти эмбеддинги вместе с исходными чанками сохраняются в векторной базе данных (например, FAISS, ChromaDB, Pinecone, Weaviate).

2.  **Обработка запроса пользователя:**
    *   **Поиск (Retrieval):** Когда пользователь задает вопрос, он также преобразуется в векторное представление (той же моделью эмбеддингов). Затем в векторной базе данных выполняется поиск наиболее релевантных (похожих по смыслу) чанков данных.
    *   **Формирование контекста:** Найденные релевантные чанки данных (контекст) передаются в LLM вместе с исходным вопросом пользователя.
    *   **Генерация ответа (Generation):** LLM использует предоставленный контекст и свой вопрос для генерации ответа. В этом сценарии LLM могла бы либо напрямую отвечать на вопрос, основываясь на извлеченных данных, либо генерировать код Pandas (или SQL-запрос) для выполнения над этими извлеченными данными или над всем датасетом, но уже с "подсказками" из релевантных частей.

**Преимущества RAG в данном контексте (потенциальные):**

*   **Более "умный" доступ к данным:** LLM получает не просто схему, а конкретные фрагменты данных, относящиеся к вопросу.
*   **Снижение галлюцинаций:** Ответы основываются на реальных данных, извлеченных на шаге Retrieval.
*   **Адаптивность:** Система может лучше справляться с вопросами о специфических значениях или комбинациях, которые сложно заранее описать в промпте со схемой.

**Сложности RAG:**

*   **Качество поиска:** Эффективность RAG сильно зависит от качества эмбеддингов и алгоритма поиска.
*   **Размер контекста LLM:** Количество извлеченных данных, которые можно передать в LLM, ограничено размером ее контекстного окна.
*   **Индексация и обновление:** Требуется процесс индексации данных и его обновления при изменениях в датасете.
*   **Более сложная архитектура:** Включает дополнительные компоненты (векторная БД, модели эмбеддингов).

В текущем задании, где основной фокус на агрегированной статистике и четко определенных операциях над структурированными данными, выбранный подход с LLM как транслятором в JSON-команды является более прямолинейным и контролируемым. RAG был бы более оправдан для более сложных, "поисковых" или менее структурированных запросов к данным.
## Технологический стек

*   **Язык:** Python 3.7+
*   **Анализ данных:** Pandas
*   **Взаимодействие с LLM:** DeepSeek API (через HTTP-запросы)
*   **HTTP-клиент:** `requests`
*   **Управление конфигурацией:** `python-dotenv` (для API-ключа)
*   **Виртуальное окружение:** `venv`

## Установка и запуск

1.  **Клонируйте репозиторий:**
    ```bash
    git clone <URL_вашего_репозитория>
    cd <название_папки_репозитория>
    ```

2.  **Создайте и активируйте виртуальное окружение:**
    ```bash
    python -m venv venv
    # Windows
    venv\Scripts\activate
    # macOS/Linux
    source venv/bin/activate
    ```

3.  **Установите зависимости:**
    ```bash
    pip install -r requirements.txt
    ```
    *(Предполагается, что вы создадите файл `requirements.txt` со следующим содержимым):*
    ```
    pandas
    requests
    python-dotenv
    ```

4.  **Настройте API-ключ DeepSeek:**
    *   Создайте файл `.env` в корневой директории проекта.
    *   Добавьте в него ваш API-ключ:
        ```env
        DEEPSEEK_API_KEY="ваш_реальный_api_ключ_deepseek"
        ```

5.  **Загрузите датасет:**
    *   Скачайте файл `freelancer_earnings_bd.csv` по [ссылке](https://www.kaggle.com/datasets/shohinurpervezshohan/freelancer-earnings-and-job-trends?select=freelancer_earnings_bd.csv).
    *   Поместите его в папку `archive` в корне проекта (или измените путь в файле `llm_analyzer.py` на актуальный). Структура должна быть примерно такой:
        ```
        -Freelancer-Earnings/
        ├── archive/
        │   └── freelancer_earnings_bd.csv
        ├── cli_app.py
        ├── llm_analyzer.py
        ├── .env
        └── requirements.txt
        ```

6.  **Запустите приложение:**
    ```bash
    python cli_app.py
    ```

## Пример использования

После запуска приложения вы увидите приглашение:

# Примеры ответов 
 
Прототип системы анализа данных о фрилансерах.
Введите ваш вопрос или 'выход' для завершения.
> Насколько выше доход у фрилансеров, принимающих оплату в криптовалюте, по сравнению с другими способами оплаты?

Обработка запроса с помощью LLM...
LLM сгенерировала JSON:
{
  "operation_type": "compare_average",
  "parameters": {
    "measure_column": "Earnings_USD",
    "category_column": "Payment_Method",
    "target_category_value": "Crypto"
  }
}

Результат анализа:
Среднее значение 'Earnings_USD' для 'Payment_Method' = 'Crypto': 5,139.30
Среднее значение 'Earnings_USD' для других категорий в 'Payment_Method': 4,973.99
Значение для 'Crypto' в среднем на 165.31 (3.32%) выше.
--------------------------------------------------
> Как распределяется доход фрилансеров в зависимости от региона проживания?

Обработка запроса с помощью LLM...
LLM сгенерировала JSON:
{
  "operation_type": "group_by_aggregate",
  "parameters": {
    "group_by_column": "Client_Region",
    "aggregate_column": "Earnings_USD",
    "aggregations": [
      "mean",
      "median",
      "sum",
      "count"
    ]
  }
}

Результат анализа:
Агрегированные данные для 'Earnings_USD' по группам из 'Client_Region':
                      mean  median      sum  count
Client_Region                                     
Asia           5172.284698  5297.0  1453412    281
Australia      4966.097315  5296.0  1479897    298
Canada         5350.134146  5435.5  1316133    246
Europe         4890.530534  5000.0  1281319    262
Middle East    4870.817869  4971.0  1417408    291
UK             5047.089286  4872.5  1413185    280
USA            4872.948630  4643.5  1422901    292
--------------------------------------------------
> Какой процент фрилансеров, считающих себя экспертами, выполнил менее 100 проектов

Обработка запроса с помощью LLM...
LLM сгенерировала JSON:
{
  "operation_type": "filter_and_calculate_percentage",
  "parameters": {
    "base_filter_column": "Experience_Level",
    "base_filter_value": "Expert",
    "condition_column": "Job_Completed",
    "condition_operator": "<",
    "condition_value": 100,
    "value_is_numeric": true
  }
}

Результат анализа:
Из 641 записей, где 'Experience_Level' = 'Expert', 217 удовлетворяют условию 'Job_Completed < 100'. Это составляет 33.85%.
--------------------------------------------------
> 



**Примеры вопросов:**

*   `Насколько выше доход у фрилансеров, принимающих оплату в криптовалюте, по сравнению с другими способами оплаты?`
*   `Как распределяется доход фрилансеров в зависимости от региона проживания?` (система поймет, что речь о `Client_Region`)
*   `Какой процент фрилансеров, считающих себя экспертами, выполнил менее 100 проектов?`
*   `Какова средняя почасовая ставка для веб-разработчиков?`
*   `Покажи минимальный и максимальный доход для каждого типа проекта.`

Система обработает ваш вопрос, выведет сгенерированный LLM JSON-запрос (для отладки) и затем результат анализа.

## Оценка системы

*   **Точность:** Для протестированных типов запросов система показала высокую точность, результаты совпадали с ручным анализом.
*   **Эффективность:** Время ответа приемлемо для CLI-прототипа, основная задержка вносится API LLM.
*   **Надежность:** Реализована базовая обработка ошибок (неверный JSON, ошибки API).

## Критерии самооценки

Качество решения оценивалось по следующим основным критериям:

1.  **Корректность интерпретации запроса LLM:** Насколько точно LLM преобразует естественный язык в структурированный JSON.
2.  **Точность аналитических результатов:** Соответствие результатов эталонным значениям.
3.  **Полнота ответа:** Способность системы предоставить всю запрашиваемую информацию.
4.  **Устойчивость:** Обработка некорректных запросов и ошибок.
5.  **Соответствие техническим требованиям задания.**

Подробный отчет с описанием подхода, оценкой эффективности и технологий можно найти [здесь](ОТЧЕТ.md) *(если вы планируете создать отдельный файл отчета)* или в соответствующем разделе документации.

## Возможные улучшения

*   Расширение набора поддерживаемых аналитических операций.
*   Улучшение промпт-инжиниринга для обработки более сложных или неоднозначных запросов (например, с использованием few-shot prompting).
*   Более продвинутая обработка ошибок и диалог с пользователем для уточнения запросов.
*   Поддержка многошаговых аналитических задач.
*   Разработка более дружелюбного пользовательского интерфейса (например, веб-интерфейса).
> 
